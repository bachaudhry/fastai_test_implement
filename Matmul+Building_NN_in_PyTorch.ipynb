{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul and Building a Neural Net in PyTorch - Understanding the Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Matrix Multiplication Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Matrix multiplications are the beating heart of Deep Learning; the so-called magic trick. Before building our Neural Net, using PyTorch, lets quickly run through a refresher of matrix multiplications using various methodologies to achieve the same results and then compare their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# URL for MNIST dataset\n",
    "MNIST_URL = \"http://deeplearning.net/data/mnist/mnist.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/bilal/.fastai/data/mnist.pkl.gz')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading dataset\n",
    "path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Loading training and validation sets directly from the pickle file\n",
    "with gzip.open(path, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " torch.Size([50000, 784]),\n",
       " tensor([5, 0, 4,  ..., 8, 4, 8]),\n",
       " torch.Size([50000]),\n",
       " tensor(0),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping the datasets into tensors and printing relevant details\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "n, c = x_train.shape\n",
    "\n",
    "x_train, x_train.shape, y_train, y_train.shape, y_train .min(), y_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating a testing function to verify details before calculations\n",
    "import operator\n",
    "\n",
    "def test(a, b, cmp, cname=None):\n",
    "    if cname is None: \n",
    "        cname = cmp.__name__\n",
    "    assert cmp(a, b), f\"{cname}:\\n{a}\\n{b}\"\n",
    "        \n",
    "\n",
    "def test_eq(a, b):\n",
    "    test(a, b, operator.eq, '==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Running tests\n",
    "assert n == y_train.shape[0] == 50000\n",
    "# Testing functions\n",
    "test_eq(c, 28*28) \n",
    "test_eq(y_train.min(), 0)\n",
    "test_eq(y_train.max(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Setting colormap for MNIST\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing image output in a 28x28 format\n",
    "img = x_train[0]\n",
    "img.view(28, 28).type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.view((28, 28)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Standard Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initializing weights and a bias vector\n",
    "weights = torch.randn(784, 10)\n",
    "bias = torch.zeros(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Doing a matrix multiplication using Python's for loops to get a sense of our slowest results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating function for matrix multiplication\n",
    "def matmul(a, b):\n",
    "    a_row, a_col = a.shape\n",
    "    b_row, b_col = b.shape\n",
    "    assert a_col == b_row\n",
    "    c = torch.zeros(a_row, b_col)\n",
    "    for i in range(a_row):\n",
    "        for j in range(b_col):\n",
    "            for k in range(a_col): # can be swapped with b_row\n",
    "                c[i, j] += a[i, k] * b[k, j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For m1, we will pick just the first 6 rows, to save time since\n",
    "# matmul in python is SUPER SLOW!\n",
    "m1 = x_valid[:5]\n",
    "m2 = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 784]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the shape of our two tensors\n",
    "m1.shape, m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 606 ms, sys: 1.12 ms, total: 607 ms\n",
      "Wall time: 524 ms\n"
     ]
    }
   ],
   "source": [
    "# Running our first matrix multiplication \n",
    "%time t1 = matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have established our baseline. Let's try to speed the whole process up by 50,000 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Element-wise Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactoring the matmul function for element wise operations\n",
    "def matmul(a, b):\n",
    "    a_row, a_col = a.shape\n",
    "    b_row, b_col = b.shape\n",
    "    assert a_col == b_row\n",
    "    c = torch.zeros(a_row, b_col)\n",
    "    for i in range(a_row):\n",
    "        for j in range(b_col):\n",
    "            c[i, j] = (a[i, :] * b[:, j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838 µs ± 161 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Running 10 loops of the matmul operation\n",
    "%timeit -n 10 _= matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This simple refactoring has resulted in a performance boost of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "798 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# If we were to export this\n",
    "# Running the torch.allclose method to test whether two elements\n",
    "# are elements-wise equal to within a tolerance\n",
    "def near(a, b):\n",
    "    return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
    "\n",
    "def test_near(a, b): \n",
    "    test(a, b, near)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing with our first matmul tensor\n",
    "test_near(t1, matmul(m1, m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Just to get a sense of what the unsqueeze() method will do, let's run a few tensor operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10., 20., 30.]), torch.Size([3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tensor([10., 20., 30.])\n",
    "c, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10., 20., 30.]]), torch.Size([3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(0), c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.],\n",
       "         [20.],\n",
       "         [30.]]),\n",
       " torch.Size([3]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(1), c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.],\n",
       "         [20.],\n",
       "         [30.]]),\n",
       " torch.Size([3]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(-1), c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now we will leverage the performance of C\n",
    "# with broadcasting to improve matmul's performance\n",
    "def matmul(a, b):\n",
    "    a_row, a_col = a.shape\n",
    "    b_row, b_col = b.shape\n",
    "    assert a_col == b_row\n",
    "    c = torch.zeros(a_row, b_col)\n",
    "    for i in range(a_row):\n",
    "        c[i] = ((a[i].unsqueeze(-1) * b).sum(dim=0))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 µs ± 53.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _= matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4475.138121546961"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "810000/181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we can see from the results above, refactoring the matmul() method using broadcasting gives us a performance increase by a whopping 4500x! \n",
    "\n",
    "But this can be improved further with PyTorch's inbuilt implementation of the Einstein Summation and the operator used for matrix multiplications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Einstein Summation and PyTorch Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The lesson NB introduces EinSum as:\n",
    "\n",
    "Einstein summation (`einsum`) is a compact representation for combining products and sums in a general way. From the numpy docs:\n",
    "\n",
    "\"The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so `np.einsum('i,i', a, b)` is equivalent to `np.inner(a,b)`. If a label appears only once, it is not summed, so `np.einsum('i', a)` produces a view of a with no changes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactoring\n",
    "def matmul(a, b):\n",
    "    return torch.einsum('ik, kj->ij', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.5 µs ± 8.14 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _= matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27739.72602739726"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "810000/29.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Performance has now gone up by 27740x!!\n",
    "\n",
    "The PyTorch operator will give us our best results yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.28 µs ± 3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 t2 = m1.matmul(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The same operation can be run with \n",
    "t2 = m1@m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_near(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81000.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "810000/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The final result is an approximate performance increase of **81,000** times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Building a Neural Network From Scratch in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The lesson notebook uses exports from the previous notebooks for\n",
    "# all its library imports.\n",
    "# We won't have to do that for now, so proceeding with loading our datasets.\n",
    "def get_data():\n",
    "    # Loads MNIST dataset\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as file:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(file, encoding='latin-1')\n",
    "        return map(tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the data sets using broadcasting\n",
    "def normalize(x, mean, std):\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Loading training and validation datasets\n",
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before initializing our weights and biases, let's take a look\n",
    "# at the training data's mean and std. dev.\n",
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the training set and validation set\n",
    "# Making sure to normalize validation set with the\n",
    "# training set's mean.\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "\n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-6.2598e-06), tensor(1.))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing means to within a tolerance range\n",
    "def test_near_zero(a, tol=1e-3): \n",
    "    assert a.abs() < tol\n",
    "    f\"Near zero:{a}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_near_zero(x_train.mean())\n",
    "test_near_zero(1-x_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the matrices \n",
    "n, m = x_train.shape\n",
    "# No. of activations\n",
    "c= y_train.max()+1\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Basic Architecture of the Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Size of a single hidden layer\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Using simplified init\n",
    "w1 = torch.randn(m, nh) / math.sqrt(m)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh, 1) / math.sqrt(nh)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing tolerance levels\n",
    "test_near_zero(w1.mean())\n",
    "test_near_zero(w1.std() - 1 / math.sqrt(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0059), tensor(0.9924))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking validation set's means\n",
    "# mean and std have to be ~ (0, 1)\n",
    "x_valid.mean(), x_valid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating a linear function for testing\n",
    "# means\n",
    "def lin(x, weight, bias):\n",
    "    return x@weight + bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = lin(x_valid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0375), tensor(0.9509))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to rectify negative values\n",
    "def relu(x):\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Adding rectifier to the linear function\n",
    "t = relu(lin(x_valid, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3961), tensor(0.5525))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result will not be mean, std ~ (0, 1)\n",
    "# since the rectifier removed all negative\n",
    "# values\n",
    "t.mean(), t.std()\n",
    "# The paper in the link goes into much detail\n",
    "# regarding this issue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From pytorch docs: `a: the negative slope of the rectifier used after this layer (0 for ReLU by default)`\n",
    "\n",
    "$$\\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan_in}}}$$\n",
    "\n",
    "This was introduced in the paper that described the Imagenet-winning approach from *He et al*: [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Using Kaiming / He init for relu\n",
    "w1 = torch.randn(m , nh) * math.sqrt(2/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0002), tensor(0.0503))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5729), tensor(0.8323))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that the weight is initalized with he init\n",
    "# The resulting std is close to one\n",
    "# However, the mean is still not close to 0\n",
    "t = relu(lin(x_valid, w1, b1))\n",
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0122,  0.0919,  0.0872,  ...,  0.0343, -0.0835, -0.0183],\n",
       "        [ 0.0692, -0.0320, -0.0400,  ...,  0.0459,  0.0290, -0.0425],\n",
       "        [ 0.0103, -0.0218, -0.0194,  ...,  0.0441, -0.0318, -0.0557],\n",
       "        ...,\n",
       "        [ 0.0042, -0.0861, -0.0370,  ..., -0.0583, -0.0047,  0.0725],\n",
       "        [ 0.0496, -0.0133,  0.1142,  ...,  0.0309,  0.0447,  0.0544],\n",
       "        [-0.0236,  0.0444,  0.0336,  ...,  0.0442, -0.0055, -0.0492]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if pytorch's he init\n",
    "# returns different or similar results\n",
    "from torch.nn import init\n",
    "\n",
    "w1 = torch.zeros(m ,nh)\n",
    "# 'fan_in' and 'fan_in' documentation states\n",
    "# Choosing 'an_in'preserves the magnitude of the variance \n",
    "# of the weights in the forward pass\n",
    "# 'fan_out' preserves the magnitudes in thebackwards pass.\n",
    "init.kaiming_normal_(w1, mode='fan_out') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fan_in'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'leaky_relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fan_in'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'leaky_relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Fills the input `Tensor` with values according to the method\u001b[0m\n",
       "\u001b[0;34m    described in `Delving deep into rectifiers: Surpassing human-level\u001b[0m\n",
       "\u001b[0;34m    performance on ImageNet classification` - He, K. et al. (2015), using a\u001b[0m\n",
       "\u001b[0;34m    normal distribution. The resulting tensor will have values sampled from\u001b[0m\n",
       "\u001b[0;34m    :math:`\\mathcal{N}(0, \\text{std}^2)` where\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. math::\u001b[0m\n",
       "\u001b[0;34m        \\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan\\_in}}}\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Also known as He initialization.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        tensor: an n-dimensional `torch.Tensor`\u001b[0m\n",
       "\u001b[0;34m        a: the negative slope of the rectifier used after this layer (0 for ReLU\u001b[0m\n",
       "\u001b[0;34m            by default)\u001b[0m\n",
       "\u001b[0;34m        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\u001b[0m\n",
       "\u001b[0;34m            preserves the magnitude of the variance of the weights in the\u001b[0m\n",
       "\u001b[0;34m            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\u001b[0m\n",
       "\u001b[0;34m            backwards pass.\u001b[0m\n",
       "\u001b[0;34m        nonlinearity: the non-linear function (`nn.functional` name),\u001b[0m\n",
       "\u001b[0;34m            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples:\u001b[0m\n",
       "\u001b[0;34m        >>> w = torch.empty(3, 5)\u001b[0m\n",
       "\u001b[0;34m        >>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_calculate_correct_fan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonlinearity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.7/site-packages/torch/nn/init.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init.kaiming_normal_??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0001), tensor(0.0507), torch.Size([784, 50]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std(), w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = relu(lin(x_valid, w1, b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**NOTE**: _The linear layer does not contain a transpose of w1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4950), tensor(0.7835))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results are pretty similar to the manual approach\n",
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a single layer NN\n",
    "torch.nn.Linear(m, nh).weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**NOTE:** _PyTorch's version of the linear layer **does** contain a transpose of w1._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Experimenting with a modification in the relu function\n",
    "# to reduce the mean of the linear function\n",
    "def relu(x):\n",
    "    return x.clamp_min(0.) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1001), tensor(0.8604))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrying initialization\n",
    "w1 = torch.randn(m, nh) * math.sqrt(2./ m)\n",
    "t1 = relu(lin(x_valid, w1, b1))\n",
    "t1.mean(), t1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to create our model\n",
    "def model(x):\n",
    "    layer1 = lin(x, w1, b1)\n",
    "    layer2 = relu(layer1)\n",
    "    layer3 = lin(layer2, w2, b2)\n",
    "    return layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.87 ms ± 182 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _= model(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing the output shape of our model\n",
    "assert model(x_valid).shape == torch.Size([x_valid.shape[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Creating MSE Loss Function for Testing\n",
    "\n",
    "For testing we will use MSE, instead of Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mse(out, targ):\n",
    "    # Getting rid of unit axis with squeeze\n",
    "    # Taking care mention the dimension\n",
    "    # in case there is ever a batch size of size 1\n",
    "    return (out.squeeze(-1) - targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Converting targets to float to calculate the \n",
    "# mse\n",
    "y_train, y_valid = y_train.float(), y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.8544)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating loss\n",
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Architecture for the Backward Pass and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Working backwards from our loss calculating layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ):\n",
    "    # Gradient loss with respect to output of previous layer\n",
    "    # storing as an attribute of the input layer \n",
    "    inp.g  = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    # Gradient of relu with respect to input activations\n",
    "    # out.g becomes the gradient of the next layer\n",
    "    inp.g = (inp > 0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):  \n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass\n",
    "    layer1 = inp @ w1 + b1\n",
    "    layer2 = relu(layer1)\n",
    "    out = layer2 @ w2 + b2\n",
    "    # This loss doesn't appear in the gradients\n",
    "    # Just so we can print it\n",
    "    loss = mse(out, targ)\n",
    "    \n",
    "    # backward pass\n",
    "    mse_grad(out, targ)\n",
    "    lin_grad(layer2, out, w2, b2)\n",
    "    relu_grad(layer1, layer2)\n",
    "    lin_grad(inp, layer1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cloning the weights and biases of the gradients for testing\n",
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "inp_g = x_train.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Using Pytorch autograd to check out results\n",
    "xt2 = x_train.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating a forward pass for testing the values cloned above\n",
    "def forward(inp, targ):\n",
    "    layer1 = inp @ w12 + b12\n",
    "    layer2 = relu(layer1)\n",
    "    out = layer2 @ w22 + b22\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test loss from the cloned training set\n",
    "test_loss = forward(xt2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing the gradients\n",
    "test_near(w22.grad, w2g)\n",
    "test_near(b22.grad, b2g)\n",
    "test_near(w12.grad, w1g)\n",
    "test_near(b12.grad, b1g)\n",
    "test_near(xt2.grad, inp_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Refactoring the Model\n",
    "\n",
    "This becomes very similar to the PyTorch API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactored version of the relu layer\n",
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.) - 0.5\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = (self.inp > 0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactored version of the linear layer\n",
    "class Lin():\n",
    "    def __init__(self, w, b): \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp @ self.w + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactored version of the MSE\n",
    "class MSE():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers:\n",
    "            x = l(x) # Function composition\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Setting all of our previously initialized gradients to zero\n",
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "# Creating model\n",
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 93.1 ms, sys: 7.16 ms, total: 100 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.99 s, sys: 2.32 s, total: 8.31 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Running tolerance test for gradients\n",
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(inp_g, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Creating Module Class to Reduce Code Duplication - with additional refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Module():\n",
    "    # This class will factor out all of the duplication\n",
    "    # and works just like nn.Module\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self):\n",
    "        raise Exception(\"Not Implemented\")\n",
    "        \n",
    "    def backward(self):\n",
    "        self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp):\n",
    "        return inp.clamp_min(0.) - 0.5\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = (inp > 0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp @ self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MSE(Module):\n",
    "    def forward(self, inp, targ):\n",
    "        return (inp.squeeze() - targ).pow(2).mean()\n",
    "    \n",
    "    def bwd(self, out, inp, targ):\n",
    "        inp.g = 2*(inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Resetting the gradients\n",
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 133 ms, sys: 1.92 ms, total: 135 ms\n",
      "Wall time: 22.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 280 ms, sys: 80 ms, total: 360 ms\n",
      "Wall time: 60.1 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our model's performance has increased considerably over the previous refactoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Doing away with einsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w ,b):\n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 9.25 ms, total: 126 ms\n",
      "Wall time: 20.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 260 ms, sys: 94.7 ms, total: 354 ms\n",
      "Wall time: 59 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Mini-Batches and the Training Loop Using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Setting output colormap\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading data using the method we created earlier\n",
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of matrices, number of activations\n",
    "# and number of hidden layers\n",
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisiting nn.Module\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions\n",
    "preds = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "Previously, we used MSE as our loss function due to expediency. Since this is a classification problem, we will use Cross-Entropy Loss.\n",
    " \n",
    " We begin by computing the softmax of our activations:\n",
    " \n",
    " $$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "We will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return (x.exp() / (x.exp().sum(-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sm = log_softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "But since our $x$s are 1-hot encoded, this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To illustrate the above, let's take 4 target values from  our training\n",
    "# set.\n",
    "y_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2128, -2.2520, -2.2651, -2.2990], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using integer array indexing, we will generate\n",
    "# loss on these predictions \n",
    "preds_sm[[0, 1, 2, 3], [5, 0, 4, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another interpretation\n",
    "def neg_ll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = neg_ll(preds_sm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2950, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula\n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax, previously defined as :\n",
    "\n",
    "`(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the formula above and \n",
    "# redefining log_softmax()\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our test_near function from before\n",
    "test_near(neg_ll(log_softmax(preds), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more stable method of computing the log of the sum of exponentials is called the _LogSumExp_ trick.\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return  m + (x - m[:, None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, using the test_near function\n",
    "test_near(logsumexp(preds), preds.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying this in log_softmax\n",
    "def log_softmax(x):\n",
    "    return x - x.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our custom loss function against PyTorch's\n",
    "# implementation\n",
    "test_near(F.nll_loss(F.log_softmax(preds, -1), y_train), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch combines F.log_softmax and F.nll_loss in\n",
    "# one optimized function i.e. F.cross_entropy\n",
    "test_near(F.cross_entropy(preds, y_train), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.2950, grad_fn=<NllLossBackward>),\n",
       " tensor(2.2950, grad_fn=<NegBackward>))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, simply\n",
    "F.cross_entropy(preds, y_train), loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basic Training Loop\n",
    "\n",
    "The training loop which we are about to write repeats the following steps:\n",
    "\n",
    "- Get the output of the model on a batch of inputs.\n",
    "- Compare the output to the labels and compute a loss.\n",
    "- Calculate the gradients of the loss with respect to every parameter of the model.\n",
    "- Update parameters with those gradients to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's optimized loss function\n",
    "loss_f = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an accuracy metric\n",
    "def accuracy(output, batch_label):\n",
    "    return (torch.argmax(output, dim=1)==batch_label).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1046,  0.0413,  0.0886, -0.0100,  0.1846,  0.1319, -0.0299, -0.1473,\n",
       "          0.1533, -0.1600], grad_fn=<SelectBackward>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "bs = 64\n",
    "# Mini batch from x\n",
    "xb = x_train[0 : bs]\n",
    "# Predictions\n",
    "preds = model(xb)\n",
    "# Predictions for a single instance\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3119, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mini Batch from y\n",
    "yb =  y_train[0 : bs]\n",
    "# Loss on predictions\n",
    "loss_f(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of predictions\n",
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we begin constructing our loop\n",
    "# Let's set our learning rate and the number of epochs\n",
    "lr = 0.5\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Basic Version of the Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs +1 ): #Go through every row till the batch size\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_f(model(xb), yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, 'weight'): # Checking if layer has weights inits\n",
    "                    layer.weight -= layer.weight.grad * lr\n",
    "                    layer.bias -= layer.bias.grad * lr\n",
    "                    layer.weight.grad.zero_() # Zeroing gradients so they don't accumulate\n",
    "                    layer.bias.grad.zero_()   # with every mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0558, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring and Using Parameters \n",
    "\n",
    "We will re-factor the Model class by using `nn.Module.__setattr__` and add ReLU to `__call__()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the model\n",
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# We can print out the particulars of each layer of our model using\n",
    "# the named_children() method\n",
    "for name, layer in model.named_children():\n",
    "    print(f\"{name}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "   (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "   (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       " ),\n",
       " Linear(in_features=784, out_features=50, bias=True),\n",
       " Linear(in_features=50, out_features=10, bias=True))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or simply\n",
    "model, model.l1, model.l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refactor the training loop by using model.parameters()\n",
    "# to simplify our code\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_f(model(xb), yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr # Simplified the whole section\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2428, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behind the scenes, PyTorch overrides the __setattr__ function in nn.Module\n",
    "# so that the submodules defined by us are properly registered as parameters\n",
    "# of the model.\n",
    "class DummyModule():\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __setattr__(self, key, value):\n",
    "        if not key.startswith(\"_\"):\n",
    "            self._modules[key] = value\n",
    "        super().__setattr__(key, value)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for layer in self._modules.values():\n",
    "            for parameter in layer.parameters():\n",
    "                yield parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = DummyModule(m, nh, 10)\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in test_model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Modules\n",
    "\n",
    "If we opt for the original **layers** approach, we will have to register the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        # Passing the layers\n",
    "        self.layers = layers\n",
    "        # Registering the modules\n",
    "        for i, l in enumerate(self.layers):\n",
    "            self.add_module(f'layer_{i}', l)\n",
    "            \n",
    "        def __call__(self, x):\n",
    "            for l in self.layers:\n",
    "                x=l(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using nn.ModuleList for the Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers) # Simpler than before\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1679, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using nn.Sequential for Even More Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0837, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "We will replace the previous manually coded optimization step in fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually creating an optimizer class before we use PyTorch's optimizers\n",
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= p.grad * self.lr\n",
    "                \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further simplifying the training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_f(pred, yb)\n",
    "        \n",
    "        # Rewritten\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1334, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement PyTorch's `optim.SGD` which has the same functionality as `Optimizer()`, but also handles additional hyper-parameters like **momentum**, **weight decay**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3180, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_f(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0536, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Processing Mini-Batches Using a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through minibatches of x and y values separately is a less than ideal process:\n",
    "\n",
    "```python\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y, = x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data sets using the new Dataset class\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "# Verifying\n",
    "assert len(train_ds) == len(x_train)\n",
    "assert len(valid_ds) == len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing a minibatch\n",
    "xb, yb = train_ds[0:5]\n",
    "\n",
    "assert xb.shape == (5, 28*28)\n",
    "assert yb.shape == (5,)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating model and optimizer\n",
    "model, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        xb, yb = train_ds[i*bs : i*bs+bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_f(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1509, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoader()\n",
    "\n",
    "The DataLoader will clean up the bit where we iterated through our batches:\n",
    "\n",
    "```python\n",
    "    for i in range((n-1)//bs+1):\n",
    "        xb, yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoader\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): \n",
    "        self.ds, self.bs = ds, bs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs):\n",
    "            yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "assert xb.shape==(bs, 28*28)\n",
    "assert yb.shape==(bs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[1].view(28, 28))\n",
    "yb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the DataLoader to the test\n",
    "model, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a fit() method based on the latest training loop\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_f(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1023, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling for the Training Set\n",
    "\n",
    "Training sets need to be in random order which differs for each iteration.\n",
    "\n",
    "**This does not apply to validation sets!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n, self.bs, self.shuffle = len(ds), bs, shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): \n",
    "            yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a small batch to test the sampler\n",
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without shuffling\n",
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 4, 7]), tensor([3, 8, 6]), tensor([1, 5, 9]), tensor([2])]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With shuffling\n",
    "s = Sampler(small_ds, 3, True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collate() method\n",
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds, self.sampler, self.collate_fn = ds, sampler, collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler:\n",
    "            yield self.collate_fn([self.ds[i] for i in s]) # This yield, coupled with the co-routine in the sampler is an\n",
    "                                                           # example of stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[1].view(28, 28))\n",
    "yb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Since the training set is shuffled, re-running this cell will generate a different\n",
    "# output each time\n",
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[1].view(28, 28))\n",
    "yb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model and optimizer\n",
    "model, opt = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debug since this cell generates an assertion error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3007, grad_fn=<NllLossBackward>), tensor(0.1250))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)\n",
    "#assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch's Data Loader\n",
    "\n",
    "As always, PyTorch's implementations are highly optimized and come with all the bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0409, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can set the parameter `drop_last` to True, and it will drop the last incomplete batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model()\n",
    "fit()\n",
    "\n",
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0518, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert acc>0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Validation loss will be printed at the end of each epoch.\n",
    "\n",
    "Also, we always call `model.train()` before training and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_f, optimizer, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        #Handle batchnorm and dropout\n",
    "        model.train()\n",
    "        #print(model.training)\n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_f(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Evaluation phase    \n",
    "        model.eval()\n",
    "        #print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc = 0., 0.\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_f(pred, yb)\n",
    "                tot_acc += accuracy(pred, yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTE` Most libraries don't have in-cuilt contingencies for different batch sizes and how they're supposed to be implemented in the fit() function above.**\n",
    "\n",
    "This will be dealt with in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function get_dls() returns dataloaders for the training and validation sets\n",
    "\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1998) tensor(0.9384)\n",
      "1 tensor(0.1249) tensor(0.9621)\n",
      "2 tensor(0.4389) tensor(0.8935)\n",
      "3 tensor(0.0981) tensor(0.9703)\n",
      "4 tensor(0.1030) tensor(0.9696)\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model, optimizer = get_model()\n",
    "loss, accuracy = fit(5, model, loss_f, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
