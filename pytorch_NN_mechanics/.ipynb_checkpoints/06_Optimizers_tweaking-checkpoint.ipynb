{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers - When, Where and How to Tweak Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-31d4bf617d48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_08\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Code Bank/ML Library/fastai_test_implement/pytorch_NN_mechanics/exp/nb_08.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.47\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.45\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0m_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.29\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m \u001b[0mnorm_imagenette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize_chan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "from exp.nb_08 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Imagenette Data From the DataBlock NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, \n",
    "        to_float_tensor]\n",
    "bs = 128\n",
    "\n",
    "img_list = ImageList.from_files(path, tfms=tfms)\n",
    "split_data = SplitData.split_by_func(img_list, \n",
    "                                     partial(grandparent_splitter,\n",
    "                                            valid_name='val'))\n",
    "\n",
    "labels = label_by_func(split_data, parent_labeler, \n",
    "                       proc_y=CategoryProcessor())\n",
    "data = labels.to_databunch(bs, c_in=3, c_out=10, \n",
    "                           num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "nfs = [32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [partial(AvgStatsCallback, accuracy),\n",
    "             CudaCallback,\n",
    "             partial(BatchTransformXCallback, norm_imagenette)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline training with vanilla SGD\n",
    "learn, run = get_learn_run(nfs, data, lr=0.4,\n",
    "                          layer=conv_layer, cbs=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTES`**\n",
    "\n",
    "- The base PyTorch optimizer in `torch.optim` is a dictionary which stores the hyper-parameters and references to the parameters of the model we want to train in parameter groups.\n",
    "\n",
    "- It contains `step` which updates our parameters with gradients and a method `zero_grad` to detach and zero the gradients of our parameters.\n",
    "\n",
    "- We will build a more flexible equivalent from scratch. Here, the step function loops over all the parameters to execute the step using stepper functions, which we will provide when initializing the optimizer.\n",
    "\n",
    "- This will end up giving us parameter groups / layer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        # Could be a generator\n",
    "        self.param_groups = list(params)\n",
    "        # Ensuring params is a list of lists of parameter tensors\n",
    "        if not isinstance(self.param_groups[0], list):\n",
    "            self.param_groups = [self.param_groups]\n",
    "        # Creating a dictionary for individual param groups with\n",
    "        # their own references\n",
    "        self.hypers = [{**defaults} for p in self.param_groups]\n",
    "        self.steppers = listify(steppers)\n",
    "        \n",
    "    def grad_params(self):\n",
    "        return [(p, hyper) for pg, hyper in zip(self.param_groups, self.hypers)\n",
    "                for p in pg if p.grad is not None]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p, hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "            \n",
    "    def step(self): \n",
    "        # This step function doesn't do anything except carry out a\n",
    "        # composition on items we pass on, which in turn carry out\n",
    "        # their own operations. For e.g. one cycle annealing, \n",
    "        # discriminative LRs etc.\n",
    "        for p, hyper in self.grad_params():\n",
    "            compose(p, self.steppers, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To carry out SGD, via the stepper\n",
    "def sgd_step(param, lr, **kwargs):\n",
    "    param.data.add_(-lr, param.grad.data)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(Optimizer, steppers=[sgd_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTES`**\n",
    "\n",
    "- After changing the optimizer, we will need to adjust the callbacks which used the properties from the PyTorch optimizer.\n",
    "\n",
    "- Hyper-parameters are in the list of dictionaries `opt.hypers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the Recorder, ParamScheduler and LR_Find classes\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self): self.lrs, self.losses = [], []\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if not self.in_train: \n",
    "            return\n",
    "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())\n",
    "        \n",
    "    def plot_lr(self): \n",
    "        plt.plot(self.lrs)\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.losses)\n",
    "        \n",
    "    def plot(self, skip_last=0):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        n = len(losses) - skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(self.lrs[:n], losses[:n])\n",
    "        \n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    _order = 1\n",
    "    \n",
    "    def __init__(self, pname, sched_funcs):\n",
    "        self.pname, self.sched_funcs = pname, listify(sched_funcs)\n",
    "    \n",
    "    def begin_batch(self):\n",
    "        if not self.in_train:\n",
    "            return\n",
    "        fs = self.sched_funcs\n",
    "        if len(fs)==1: \n",
    "            fs = fs*len(self.opt.param_groups)\n",
    "        pos = self.n_epochs / self.epochs\n",
    "        for f, h in zip(fs, self.opt.hypers):\n",
    "            h[self.pname] = f(pos)\n",
    "                \n",
    "\n",
    "class LR_Find(Callback):\n",
    "    _order = 1\n",
    "    def __init(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr\n",
    "        self.best_loss = 1e9\n",
    "        \n",
    "    def begin_batch(self):\n",
    "        if not self.in_train: \n",
    "            return\n",
    "        pos = self.n_iter / self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr / self.min_lr) ** pos\n",
    "        for pg in self.opt.hypers:\n",
    "            pg['lr'] = lr\n",
    "            \n",
    "    def after_step(self):\n",
    "        if self.n_iter >= self.max_iter or self.loss > self.best_loss*10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss < self.best_loss:\n",
    "            self.best_loss = self.loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if the param scheduler and the recorder are working properly\n",
    "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)])\n",
    "\n",
    "callbacks = [partial(AvgStatsCallback, accuracy),\n",
    "             CudaCallback, Recorder, \n",
    "             partial(ParamScheduler, 'lr', sched)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(nfs, data, lr=0.4, layer=conv_layer,\n",
    "                           cbs=callbacks, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss\n",
    "run.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting LR\n",
    "run.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTES`**\n",
    "\n",
    "- In order to avoid over-fitting we will implement L2 regularization which is commonly known as weigth decay. Here we will be adding the sum of all the weights squared to our loss function. Doing this will ensure that the weights remain as small as possible when we compute the gradients.\n",
    "\n",
    "- Limiting the weights from growing at a high rate is going to hinder the training of the model, but it yeilds a state where it generalizes better.\n",
    "\n",
    "- Weight decay is a parameter that controls the sum of squares we add to our loss:\n",
    "```python\n",
    "loss_with_wd = loss + (wd/2) * (weights**2).sum()\n",
    "```\n",
    "\n",
    "- In practice it would be very inefficient to compute that big sum and add it to the loss. Alternatively, adding the big sum to our loss is the same as doing:\n",
    "```python\n",
    "weight.grad += wd * weight\n",
    "```\n",
    "\n",
    "- For every weight in our vanilla SGD is equivalent to updating the parameters with:\n",
    "```python\n",
    "wigh5t = weight - lr * (weight.grad + wd*weight)\n",
    "```\n",
    "\n",
    "- This results in the \"decay\" of each weight by a factor `lr * wd`. However, this only works for standard SGD, as we have seen with momentum, RMSProp and Adam, the update has some additional formulas around the gradient. In those cases the formula that comes from L2 regularization is:\n",
    "```python\n",
    "weight.grad += wd * weight\n",
    "```\n",
    "becomes different than weight decay\n",
    "```python\n",
    "new_weight = weight - lr * weight.grad - lr * wd * weight\n",
    "```\n",
    "\n",
    "- It should be noted that most libraries use the first formula, but as pointed out in the paper [Decoupled Weight Regularization](https://arxiv.org/pdf/1711.05101.pdf), it is better to use the second one with the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_decay(param, lr, wd, **kwargs):\n",
    "    param.data.mul_(1 - lr*wd)\n",
    "    return param\n",
    "\n",
    "weight_decay._defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization adds wd*weight to the gradients\n",
    "def l2_regularization(param, lr, wd, **kwargs):\n",
    "    param.grad.data.add_(wd, param.data)\n",
    "    return param\n",
    "\n",
    "l2_regularization._defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowing the steppers to add to our default hyper param values\n",
    "# The helper function adds in 'dest' the key/values it finds while going\n",
    "# through `os` and applying `f` when there was no `key` of the same name\n",
    "def maybe_update(os, dest, f):\n",
    "    for o in os:\n",
    "        for k,v in f(o).items():\n",
    "            if k not in dest:\n",
    "                dest[k] = v\n",
    "                \n",
    "def get_defaults(d):\n",
    "    return getattr(d, '_defaults', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The optimizer remains mostly the same as before, we only take the default values of the steppers when none are provided in the kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        self.steppers = listify(steppers)\n",
    "        maybe_update(self.steppers, defaults, get_defaults)\n",
    "        # might be a generator\n",
    "        self.param_groups = list(params)\n",
    "        # ensure params is a list of lists\n",
    "        if not isinstance(self.param_groups[0], list): \n",
    "            self.param_groups = [self.param_groups]\n",
    "        self.hypers = [{**defaults} for p in self.param_groups]\n",
    "\n",
    "    def grad_params(self):\n",
    "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
    "            for p in pg if p.grad is not None]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for p,hyper in self.grad_params(): \n",
    "            compose(p, self.steppers, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(nfs, data, lr=0.4, layer=conv_layer,\n",
    "                           cbs=callbacks, opt_func=sgd_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training let's check whether everythign works as intended\n",
    "# If we dont't provide a value for 'wd', we will pull the corresponding\n",
    "# default from 'weight_decay'\n",
    "model = learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = sgd_opt(model.parameters(), lr=0.1)\n",
    "test_eq(opt.hypers[0]['wd'], 0.)\n",
    "test_eq(opt.hypers[0]['lr'], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, when we provide a value, it overrides the default\n",
    "opt = sgd_opt(model.parameters(), lr=0.1, wd=1e-4)\n",
    "test_eq(opt.hypers[0]['wd'], 0.0001)\n",
    "test_eq(opt.hypers[0]['lr'], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying what we have built to our model\n",
    "cbfs = [partial(AvgStatsCallback, accuracy), CudaCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(nfs, data, 0.3, layer=conv_layer,\n",
    "                          cbs=callbacks, opt_func=partial(sgd_opt, wd=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We've already improved the baseline!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Momentum to the Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTES`**\n",
    "\n",
    "- The key factor in adding Momentum to the optimzer is adding some state.\n",
    "\n",
    "- Moving averages of the gradients need to be saved and stored inside the optimizer state.\n",
    "\n",
    "- To do this, we will be adding statistics. These are objects with two methods i.e. `init_state` which returns the initial state (a tensor of 0. for the moving average of gradients) and `update`, which updates the state with the new gradient value.\n",
    "\n",
    "- `_defaults` will also be read to provide default values to hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulOptimizer(Optimizer):\n",
    "    def __init__(self, params, steppers, stats=None, **defaults):\n",
    "        self.stats = listify(stats)\n",
    "        maybe_update(self.stats, defaults, get_defaults)\n",
    "        super().__init__(params, steppers, **defaults)\n",
    "        self.state = {}\n",
    "        \n",
    "    def step(self):\n",
    "        for param, hyper in self.grad_params():\n",
    "            if param not in self.state:\n",
    "                # Create a state for param and call the statistics to initialize it\n",
    "                self.state[param] = {}\n",
    "                maybe_update(self.stats, self.state[param],\n",
    "                             lambda o: o.init_state(param))\n",
    "            state = self.state[param]\n",
    "            for stat in self.stats:\n",
    "                state = stat.update(param, state, **hyper)\n",
    "            compose(param, self.steppers, **state, **hyper)\n",
    "            self.state[param] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats, at least the way they work and are updated, resemble steppers \n",
    "class Stat():\n",
    "    _defaults = {}\n",
    "    def init_state(self, param):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, param, state, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use momentum as an example of adding Stats\n",
    "class AverageGrad(Stat):\n",
    "    _defaults = dict(mom=0.9)\n",
    "    \n",
    "    def init_state(self, param):\n",
    "        return {'grad_avg': torch.zeros_like(param.grad.data)}\n",
    "    \n",
    "    def update(self, param, state, mom, **kwargs):\n",
    "        state['grad_avg'].mul_(mom).add_(param.grad.data) #this is momentum as per the definition\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the momentum step.\n",
    "def momentum_step(param, lr, grad_avg, **kwargs):\n",
    "    param.data.add_(-lr, grad_avg)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_mom_opt = partial(StatefulOptimizer, steppers=[momentum_step, weight_decay],\n",
    "                      stats=AverageGrad(), wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(nfs, data, lr=0.3, layer=conv_layer,\n",
    "                          cbs=callbacks, opt_func=sgd_mom_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Momentum Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we will try to figure out what momentum actually does to the gradients by generating some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-4, 4, 200)\n",
    "y = torch.randn(200) + 0.3\n",
    "betas = [0.5, 0.7, 0.9, 0.999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_momentum(f):\n",
    "    _, axs = plt.subplots(2, 2, figsize=(12,8))\n",
    "    for beta, ax in zip(betas, axs.flatten()):\n",
    "        ax.plot(y, linestyle='None', marker='.')\n",
    "        avg, res = None, []\n",
    "        for i, yi in enumerate(y):\n",
    "            avg, param = f(avg, beta, yi, i)\n",
    "            res.append(param)\n",
    "        ax.plot(res, color='red')\n",
    "        ax.set_title(f'beta={beta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using regular momentum\n",
    "def mom1(avg, beta, yi, i):\n",
    "    if avg is None:\n",
    "        avg = yi\n",
    "    res = beta*avg + yi\n",
    "    return res, res\n",
    "\n",
    "plot_momentum(mom1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular momentum can easily cause the results to diverge, especially if the value is\n",
    "# too high.\n",
    "# A better way to smooth noisy data is to do an exponentially weighted moving average.\n",
    "# Here there is a dampening of (1-beta) in front of the new value. We will define\n",
    "# lin_comb(linear combination) to make this easier.\n",
    "def lin_comb(v1, v2, beta):\n",
    "    return beta*v1 + (1-beta)*v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum2(avg, beta, yi, i):\n",
    "    if avg is None: \n",
    "        avg=yi\n",
    "    avg = lin_comb(avg, yi, beta)\n",
    "    return avg, avg\n",
    "\n",
    "plot_momentum(momentum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - This is the result when our data is purely random. If it has a certain shape, then it will get that shape, but with a delay for high beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1 - (x/3)**2 + torch.randn(200)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_momentum(momentum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the case of beta=0.999, item 1 is massively biasing the start of optimizing.\n",
    "\n",
    "- We will now introduce debiasing. Basically, we will be dividing our moving average with sum of the coefficients:\n",
    "$\\begin{align*}\n",
    "S = 1 - \\beta^{i+1}\n",
    "\\end{align*}$\n",
    "\n",
    "- By dividing by this term, we make our moving average a true average (all coefficients we used for the average sum up to 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum3(avg, beta, yi, i):\n",
    "    if avg is None:\n",
    "        avg = 0\n",
    "    avg = lin_comb(avg, yi, beta)\n",
    "    return avg, avg / (1-beta**(i+1))\n",
    "\n",
    "plot_momentum(momentum3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Adam, we use the gradient averages with dampening which won't be the same as SGD with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the necessary changes to AverageGrad\n",
    "class AverageGrad(Stat):\n",
    "    _defaults = dict(mom=0.9)\n",
    "    \n",
    "    def __init__(self, dampening: bool=False):\n",
    "        self.dampening=dampening\n",
    "    \n",
    "    def init_state(self, param):\n",
    "        return {'grad_avg': torch.zeros_like(param.grad.data)}\n",
    "    \n",
    "    def update(self, param, state, mom, **kwargs):\n",
    "        state['mom_damp'] = 1-mom if self.dampening else 1.\n",
    "        state['grad_avg'].mul_(mom).add_(state['mom_damp'], param.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also need to track the moving average of the gradients squared.\n",
    "class AverageSqrGrad(Stat):\n",
    "    _defaults = dict(sqr_mom=0.99)\n",
    "    \n",
    "    def __init__(self, dampening:bool=True):\n",
    "        self.dampening = dampening\n",
    "        \n",
    "    def init_state(self, param):\n",
    "        return {'sqr_avg': torch.zeros_like(param.grad.data)}\n",
    "    \n",
    "    def update(self, param, state, sqr_mom, **kwargs):\n",
    "        state['sqr_damp'] = 1-sqr_mom if self.dampening else 1.\n",
    "        state['sqr_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp'], \n",
    "                                                param.grad.data, param.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also need the number of steps done during training for debiasing.\n",
    "class StepCount(Stat):\n",
    "    def init_state(self, param):\n",
    "        return {'step': 0}\n",
    "    \n",
    "    def update(self, param, state, **kwargs):\n",
    "        state['step'] += 1\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function computes the debias term. If we're dampening then 'damp = 1-mom'\n",
    "# while if we're not dampening `damp=1`,we will need to divide by '1-mom' since that\n",
    "# term is missing in every calculation.\n",
    "def debias(mom, damp, step):\n",
    "    return damp * (1 - mom**step) / (1 - mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Adam step\n",
    "def adam_step(param, lr, mom, mom_damp, step, sqr_mom, sqr_damp,\n",
    "              grad_avg, sqr_avg, eps, **kwargs):\n",
    "    debias1 = debias(mom, mom_damp, step)\n",
    "    debias2 = debias(sqr_mom, sqr_damp, step)\n",
    "    param.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps) # eps inside or outside the sqrt()??\n",
    "    return param\n",
    "\n",
    "adam_step._defaults = dict(eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_opt(extra_step=None, **kwargs):\n",
    "    return partial(StatefulOptimizer, \n",
    "                   steppers=[adam_step, weight_decay]+listify(extra_step),\n",
    "                   stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()],\n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(nfs, data, lr=0.001, layer=conv_layer, cbs=callbacks,\n",
    "                          opt_func=adam_opt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(5, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the LAMB Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
