{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul and Building a Neural Net in PyTorch - Understanding the Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Matrix Multiplication Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Matrix multiplications are the beating heart of Deep Learning; the so-called magic trick. Before building our Neural Net, using PyTorch, lets quickly run through a refresher of matrix multiplications using various methodologies to achieve the same results and then compare their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# URL for MNIST dataset\n",
    "MNIST_URL = \"http://deeplearning.net/data/mnist/mnist.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/bilal/.fastai/data/mnist.pkl.gz')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading dataset\n",
    "path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Loading training and validation sets directly from the pickle file\n",
    "with gzip.open(path, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " torch.Size([50000, 784]),\n",
       " tensor([5, 0, 4,  ..., 8, 4, 8]),\n",
       " torch.Size([50000]),\n",
       " tensor(0),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping the datasets into tensors and printing relevant details\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "n, c = x_train.shape\n",
    "\n",
    "x_train, x_train.shape, y_train, y_train.shape, y_train .min(), y_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "import operator\n",
    "\n",
    "def test(a, b, cmp, cname=None):\n",
    "    if cname is None: \n",
    "        cname = cmp.__name__\n",
    "    assert cmp(a, b), f\"{cname}:\\n{a}\\n{b}\"\n",
    "        \n",
    "\n",
    "def test_eq(a, b):\n",
    "    test(a, b, operator.eq, '==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Running tests\n",
    "assert n == y_train.shape[0] == 50000\n",
    "# Testing functions\n",
    "test_eq(c, 28*28) \n",
    "test_eq(y_train.min(), 0)\n",
    "test_eq(y_train.max(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Setting colormap for MNIST\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing image output in a 28x28 format\n",
    "img = x_train[0]\n",
    "img.view(28, 28).type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.view((28, 28)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Standard Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initializing weights and a bias vector\n",
    "weights = torch.randn(784, 10)\n",
    "bias = torch.zeros(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Doing a matrix multiplication using Python's for loops to get a sense of our slowest results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating function for matrix multiplication\n",
    "def matmul(a, b):\n",
    "    a_row, a_col = a.shape\n",
    "    b_row, b_col = b.shape\n",
    "    assert a_col == b_row\n",
    "    c = torch.zeros(a_row, b_col)\n",
    "    for i in range(a_row):\n",
    "        for j in range(b_col):\n",
    "            for k in range(a_col): # can be swapped with b_row\n",
    "                c[i, j] += a[i, k] * b[k, j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For m1, we will pick just the first 6 rows, to save time since\n",
    "# matmul in python is SUPER SLOW!\n",
    "m1 = x_valid[:5]\n",
    "m2 = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 784]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the shape of our two tensors\n",
    "m1.shape, m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 533 ms, sys: 81 µs, total: 533 ms\n",
      "Wall time: 532 ms\n"
     ]
    }
   ],
   "source": [
    "# Running our first matrix multiplication \n",
    "%time t1 = matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have established our baseline. Let's try to speed the whole process up by 50,000 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Element-wise Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactoring the matmul function for element wise operations\n",
    "def matmul(a, b):\n",
    "    a_row, a_col = a.shape\n",
    "    b_row, b_col = b.shape\n",
    "    assert a_col == b_row\n",
    "    c = torch.zeros(a_row, b_col)\n",
    "    for i in range(a_row):\n",
    "        for j in range(b_col):\n",
    "            c[i, j] = (a[i, :] * b[:, j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848 µs ± 53 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Running 10 loops of the matmul operation\n",
    "%timeit -n 10 _= matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This simple refactoring has resulted in a performance boost of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "798 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# If we were to export this\n",
    "# Running the torch.allclose method to test whether two elements\n",
    "# are elements-wise equal to within a tolerance\n",
    "\n",
    "#export\n",
    "def near(a, b):\n",
    "    return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
    "\n",
    "def test_near(a, b): \n",
    "    test(a, b, near)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing with our first matmul tensor\n",
    "test_near(t1, matmul(m1, m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Just to get a sense of what the unsqueeze() method will do, let's run a few tensor operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10., 20., 30.]), torch.Size([3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tensor([10., 20., 30.])\n",
    "c, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10., 20., 30.]]), torch.Size([3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(0), c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.],\n",
       "         [20.],\n",
       "         [30.]]),\n",
       " torch.Size([3]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(1), c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.],\n",
       "         [20.],\n",
       "         [30.]]),\n",
       " torch.Size([3]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(-1), c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now we will leverage the performance of C\n",
    "# with broadcasting to improve matmul's performance\n",
    "def matmul(a, b):\n",
    "    a_row, a_col = a.shape\n",
    "    b_row, b_col = b.shape\n",
    "    assert a_col == b_row\n",
    "    c = torch.zeros(a_row, b_col)\n",
    "    for i in range(a_row):\n",
    "        c[i] = ((a[i].unsqueeze(-1) * b).sum(dim=0))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 µs ± 27.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _= matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4475.138121546961"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "810000/181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we can see from the results above, refactoring the matmul() method using broadcasting gives us a performance increase by a whopping 4500x! \n",
    "\n",
    "But this can be improved further with PyTorch's inbuilt implementation of the Einstein Summation and the operator used for matrix multiplications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Einstein Summation and PyTorch Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The lesson NB introduces EinSum as:\n",
    "\n",
    "Einstein summation (`einsum`) is a compact representation for combining products and sums in a general way. From the numpy docs:\n",
    "\n",
    "\"The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so `np.einsum('i,i', a, b)` is equivalent to `np.inner(a,b)`. If a label appears only once, it is not summed, so `np.einsum('i', a)` produces a view of a with no changes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactoring\n",
    "def matmul(a, b):\n",
    "    return torch.einsum('ik, kj->ij', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.8 µs ± 6.87 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _= matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27739.72602739726"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "810000/29.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Performance has now gone up by 27740x!!\n",
    "\n",
    "The PyTorch operator will give us our best results yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.98 µs ± 3.54 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 t2 = m1.matmul(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The same operation can be run with \n",
    "t2 = m1@m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_near(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81000.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "810000/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The final result is an approximate performance increase of **81,000** times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Building a Neural Network From Scratch in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The lesson notebook uses exports from the previous notebooks for\n",
    "# all its library imports.\n",
    "# We won't have to do that for now, so proceeding with loading our datasets.\n",
    "def get_data():\n",
    "    # Loads MNIST dataset\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as file:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(file, encoding='latin-1')\n",
    "        return map(tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the data sets using broadcasting\n",
    "def normalize(x, mean, std):\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Loading training and validation datasets\n",
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before initializing our weights and biases, let's take a look\n",
    "# at the training data's mean and std. dev.\n",
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the training set and validation set\n",
    "# Making sure to normalize validation set with the\n",
    "# training set's mean.\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "\n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1325e-08), tensor(1.))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing means to within a tolerance range\n",
    "\n",
    "#export\n",
    "def test_near_zero(a, tol=1e-3): \n",
    "    assert a.abs() < tol\n",
    "    f\"Near zero:{a}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_near_zero(x_train.mean())\n",
    "test_near_zero(1-x_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the matrices \n",
    "n, m = x_train.shape\n",
    "# No. of activations\n",
    "c= y_train.max()+1\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Basic Architecture of the Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Size of a single hidden layer\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Using simplified init\n",
    "w1 = torch.randn(m, nh) / math.sqrt(m)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh, 1) / math.sqrt(nh)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing tolerance levels\n",
    "test_near_zero(w1.mean())\n",
    "test_near_zero(w1.std() - 1 / math.sqrt(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0059), tensor(0.9924))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking validation set's means\n",
    "# mean and std have to be ~ (0, 1)\n",
    "x_valid.mean(), x_valid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating a linear function for testing\n",
    "# means\n",
    "def lin(x, weight, bias):\n",
    "    return x@weight + bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = lin(x_valid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0075), tensor(1.0399))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to rectify negative values\n",
    "def relu(x):\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Adding rectifier to the linear function\n",
    "t = relu(lin(x_valid, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4062), tensor(0.6051))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result will not be mean, std ~ (0, 1)\n",
    "# since the rectifier removed all negative\n",
    "# values\n",
    "t.mean(), t.std()\n",
    "# The paper in the link goes into much detail\n",
    "# regarding this issue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From pytorch docs: `a: the negative slope of the rectifier used after this layer (0 for ReLU by default)`\n",
    "\n",
    "$$\\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan_in}}}$$\n",
    "\n",
    "This was introduced in the paper that described the Imagenet-winning approach from *He et al*: [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Using Kaiming / He init for relu\n",
    "w1 = torch.randn(m , nh) * math.sqrt(2/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.0111e-05), tensor(0.0505))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5448), tensor(0.7948))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that the weight is initalized with he init\n",
    "# The resulting std is close to one\n",
    "# However, the mean is still not close to 0\n",
    "t = relu(lin(x_valid, w1, b1))\n",
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0066,  0.0055,  0.0336,  ...,  0.0603,  0.0038, -0.0109],\n",
       "        [-0.0188, -0.0333,  0.0173,  ...,  0.0104,  0.0005,  0.0419],\n",
       "        [ 0.0447,  0.0291,  0.0194,  ...,  0.0599,  0.0757,  0.0281],\n",
       "        ...,\n",
       "        [-0.0508,  0.0997,  0.1045,  ..., -0.0631, -0.1321,  0.0195],\n",
       "        [ 0.0112,  0.1287,  0.0236,  ...,  0.0412, -0.0247,  0.0549],\n",
       "        [ 0.0507, -0.0432, -0.0449,  ..., -0.0008,  0.0108,  0.0766]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if pytorch's he init\n",
    "# returns different or similar results\n",
    "\n",
    "#export\n",
    "from torch.nn import init\n",
    "\n",
    "w1 = torch.zeros(m ,nh)\n",
    "# 'fan_in' and 'fan_in' documentation states\n",
    "# Choosing 'an_in'preserves the magnitude of the variance \n",
    "# of the weights in the forward pass\n",
    "# 'fan_out' preserves the magnitudes in thebackwards pass.\n",
    "init.kaiming_normal_(w1, mode='fan_out') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init.kaiming_normal_??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.7766e-05), tensor(0.0502), torch.Size([784, 50]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std(), w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = relu(lin(x_valid, w1, b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**NOTE**: _The linear layer does not contain a transpose of w1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5481), tensor(0.8374))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results are pretty similar to the manual approach\n",
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a single layer NN\n",
    "torch.nn.Linear(m, nh).weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**NOTE:** _PyTorch's version of the linear layer **does** contain a transpose of w1._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Experimenting with a modification in the relu function\n",
    "# to reduce the mean of the linear function\n",
    "def relu(x):\n",
    "    return x.clamp_min(0.) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1839), tensor(0.9382))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrying initialization\n",
    "w1 = torch.randn(m, nh) * math.sqrt(2./ m)\n",
    "t1 = relu(lin(x_valid, w1, b1))\n",
    "t1.mean(), t1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to create our model\n",
    "def model(x):\n",
    "    layer1 = lin(x, w1, b1)\n",
    "    layer2 = relu(layer1)\n",
    "    layer3 = lin(layer2, w2, b2)\n",
    "    return layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.32 ms ± 429 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _= model(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing the output shape of our model\n",
    "assert model(x_valid).shape == torch.Size([x_valid.shape[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Creating MSE Loss Function for Testing\n",
    "\n",
    "For testing we will use MSE, instead of Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def mse(out, targ):\n",
    "    # Getting rid of unit axis with squeeze\n",
    "    # Taking care mention the dimension\n",
    "    # in case there is ever a batch size of size 1\n",
    "    return (out.squeeze(-1) - targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Converting targets to float to calculate the \n",
    "# mse\n",
    "y_train, y_valid = y_train.float(), y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.3689)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating loss\n",
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Architecture for the Backward Pass and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Working backwards from our loss calculating layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ):\n",
    "    # Gradient loss with respect to output of previous layer\n",
    "    # storing as an attribute of the input layer \n",
    "    inp.g  = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    # Gradient of relu with respect to input activations\n",
    "    # out.g becomes the gradient of the next layer\n",
    "    inp.g = (inp > 0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):  \n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass\n",
    "    layer1 = inp @ w1 + b1\n",
    "    layer2 = relu(layer1)\n",
    "    out = layer2 @ w2 + b2\n",
    "    # This loss doesn't appear in the gradients\n",
    "    # Just so we can print it\n",
    "    loss = mse(out, targ)\n",
    "    \n",
    "    # backward pass\n",
    "    mse_grad(out, targ)\n",
    "    lin_grad(layer2, out, w2, b2)\n",
    "    relu_grad(layer1, layer2)\n",
    "    lin_grad(inp, layer1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cloning the weights and biases of the gradients for testing\n",
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "inp_g = x_train.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Using Pytorch autograd to check out results\n",
    "xt2 = x_train.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating a forward pass for testing the values cloned above\n",
    "def forward(inp, targ):\n",
    "    layer1 = inp @ w12 + b12\n",
    "    layer2 = relu(layer1)\n",
    "    out = layer2 @ w22 + b22\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test loss from the cloned training set\n",
    "test_loss = forward(xt2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing the gradients\n",
    "test_near(w22.grad, w2g)\n",
    "test_near(b22.grad, b2g)\n",
    "test_near(w12.grad, w1g)\n",
    "test_near(b12.grad, b1g)\n",
    "test_near(xt2.grad, inp_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Refactoring the Model\n",
    "\n",
    "This becomes very similar to the PyTorch API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactored version of the relu layer\n",
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.) - 0.5\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = (self.inp > 0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactored version of the linear layer\n",
    "class Lin():\n",
    "    def __init__(self, w, b): \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp @ self.w + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Refactored version of the MSE\n",
    "class MSE():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers:\n",
    "            x = l(x) # Function composition\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Setting all of our previously initialized gradients to zero\n",
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "# Creating model\n",
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 ms, sys: 4.53 ms, total: 147 ms\n",
      "Wall time: 24.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.24 s, sys: 2.67 s, total: 8.91 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Running tolerance test for gradients\n",
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(inp_g, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Creating Module Class to Reduce Code Duplication - with additional refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Module():\n",
    "    # This class will factor out all of the duplication\n",
    "    # and works just like nn.Module\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self):\n",
    "        raise Exception(\"Not Implemented\")\n",
    "        \n",
    "    def backward(self):\n",
    "        self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp):\n",
    "        return inp.clamp_min(0.) - 0.5\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = (inp > 0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp @ self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MSE(Module):\n",
    "    def forward(self, inp, targ):\n",
    "        return (inp.squeeze() - targ).pow(2).mean()\n",
    "    \n",
    "    def bwd(self, out, inp, targ):\n",
    "        inp.g = 2*(inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Resetting the gradients\n",
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 107 ms, sys: 2.85 ms, total: 110 ms\n",
      "Wall time: 18.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 242 ms, sys: 124 ms, total: 366 ms\n",
      "Wall time: 61.1 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our model's performance has increased considerably over the previous refactoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Doing away with einsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w ,b):\n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 145 ms, sys: 301 µs, total: 146 ms\n",
      "Wall time: 24.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 226 ms, sys: 113 ms, total: 338 ms\n",
      "Wall time: 56.7 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batches and the Training Loop Using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Setting output colormap\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading data using the method we created earlier\n",
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of matrices, number of activations\n",
    "# and number of hidden layers\n",
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisiting nn.Module\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions\n",
    "preds = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "Previously, we used MSE as our loss function due to expediency. Since this is a classification problem, we will use Cross-Entropy Loss.\n",
    " \n",
    " We begin by computing the softmax of our activations:\n",
    " \n",
    " $$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "We will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return (x.exp() / (x.exp().sum(-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sm = log_softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "But since our $x$s are 1-hot encoded, this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To illustrate the above, let's take 4 target values from  our training\n",
    "# set.\n",
    "y_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.1390, -2.1414, -2.3586, -2.4629], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using integer array indexing, we will generate\n",
    "# loss on these predictions \n",
    "preds_sm[[0, 1, 2, 3], [5, 0, 4, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another interpretation\n",
    "def neg_ll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = neg_ll(preds_sm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3088, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula\n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax, previously defined as :\n",
    "\n",
    "`(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the formula above and \n",
    "# redefining log_softmax()\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our test_near function from before\n",
    "test_near(neg_ll(log_softmax(preds), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more stable method of computing the log of the sum of exponentials is called the _LogSumExp_ trick.\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return  m + (x - m[:, None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, using the test_near function\n",
    "test_near(logsumexp(preds), preds.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying this in log_softmax\n",
    "def log_softmax(x):\n",
    "    return x - x.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our custom loss function against PyTorch's\n",
    "# implementation\n",
    "test_near(F.nll_loss(F.log_softmax(preds, -1), y_train), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch combines F.log_softmax and F.nll_loss in\n",
    "# one optimized function i.e. F.cross_entropy\n",
    "test_near(F.cross_entropy(preds, y_train), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3088, grad_fn=<NllLossBackward>),\n",
       " tensor(2.3088, grad_fn=<NegBackward>))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, simply\n",
    "F.cross_entropy(preds, y_train), loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basic Training Loop\n",
    "\n",
    "The training loop which we are about to write repeats the following steps:\n",
    "\n",
    "- Get the output of the model on a batch of inputs.\n",
    "- Compare the output to the labels and compute a loss.\n",
    "- Calculate the gradients of the loss with respect to every parameter of the model.\n",
    "- Update parameters with those gradients to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's optimized loss function\n",
    "loss_f = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an accuracy metric\n",
    "def accuracy(output, batch_label):\n",
    "    return (torch.argmax(output, dim=1)==batch_label).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1394, -0.1354, -0.1631, -0.0741, -0.1674,  0.1472, -0.0934,  0.0348,\n",
       "          0.0443,  0.0395], grad_fn=<SelectBackward>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "bs = 64\n",
    "# Mini batch from x\n",
    "xb = x_train[0 : bs]\n",
    "# Predictions\n",
    "preds = model(xb)\n",
    "# Predictions for a single instance\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3133, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mini Batch from y\n",
    "yb =  y_train[0 : bs]\n",
    "# Loss on predictions\n",
    "loss_f(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1250)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of predictions\n",
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we begin constructing our loop\n",
    "# Let's set our learning rate and the number of epochs\n",
    "lr = 0.5\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Basic Version of the Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs +1 ): #Go through every row till the batch size\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_f(model(xb), yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, 'weight'): # Checking if layer has weights inits\n",
    "                    layer.weight -= layer.weight.grad * lr\n",
    "                    layer.bias -= layer.bias.grad * lr\n",
    "                    layer.weight.grad.zero_() # Zeroing gradients so they don't accumulate\n",
    "                    layer.bias.grad.zero_()   # with every mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3178, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8750)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring and Using Parameters \n",
    "\n",
    "We will re-factor the Model class by using `nn.Module.__setattr__` and add ReLU to `__call__()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the model\n",
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# We can print out the particulars of each layer of our model using\n",
    "# the named_children() method\n",
    "for name, layer in model.named_children():\n",
    "    print(f\"{name}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "   (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "   (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       " ),\n",
       " Linear(in_features=784, out_features=50, bias=True),\n",
       " Linear(in_features=50, out_features=10, bias=True))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or simply\n",
    "model, model.l1, model.l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refactor the training loop by using model.parameters()\n",
    "# to simplify our code\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_f(model(xb), yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr # Simplified the whole section\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1327, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behind the scenes, PyTorch overrides the __setattr__ function in nn.Module\n",
    "# so that the submodules defined by us are properly registered as parameters\n",
    "# of the model.\n",
    "class DummyModule():\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __setattr__(self, key, value):\n",
    "        if not key.startswith(\"_\"):\n",
    "            self._modules[key] = value\n",
    "        super().__setattr__(key, value)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for layer in self._modules.values():\n",
    "            for parameter in layer.parameters():\n",
    "                yield parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = DummyModule(m, nh, 10)\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in test_model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Modules\n",
    "\n",
    "If we opt for the original **layers** approach, we will have to register the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        # Passing the layers\n",
    "        self.layers = layers\n",
    "        # Registering the modules\n",
    "        for i, l in enumerate(self.layers):\n",
    "            self.add_module(f'layer_{i}', l)\n",
    "            \n",
    "        def __call__(self, x):\n",
    "            for l in self.layers:\n",
    "                x=l(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using nn.ModuleList for the Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers) # Simpler than before\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0751, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using nn.Sequential for Even More Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0970, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "We will replace the previous manually coded optimization step in fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually creating an optimizer class before we use PyTorch's optimizers\n",
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= p.grad * self.lr\n",
    "                \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further simplifying the training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_f(pred, yb)\n",
    "        \n",
    "        # Rewritten\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1128, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement PyTorch's `optim.SGD` which has the same functionality as `Optimizer()`, but also handles additional hyper-parameters like **momentum**, **weight decay**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3257, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_f(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1908, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Mini-Batches Using a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through minibatches of x and y values separately is a less than ideal process:\n",
    "\n",
    "```python\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y, = x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data sets using the new Dataset class\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "# Verifying\n",
    "assert len(train_ds) == len(x_train)\n",
    "assert len(valid_ds) == len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing a minibatch\n",
    "xb, yb = train_ds[0:5]\n",
    "\n",
    "assert xb.shape == (5, 28*28)\n",
    "assert yb.shape == (5,)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating model and optimizer\n",
    "model, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        xb, yb = train_ds[i*bs : i*bs+bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_f(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1200, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoader()\n",
    "\n",
    "The DataLoader will clean up the bit where we iterated through our batches:\n",
    "\n",
    "```python\n",
    "    for i in range((n-1)//bs+1):\n",
    "        xb, yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoader\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): \n",
    "        self.ds, self.bs = ds, bs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs):\n",
    "            yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "assert xb.shape==(bs, 28*28)\n",
    "assert yb.shape==(bs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[1].view(28, 28))\n",
    "yb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the DataLoader to the test\n",
    "model, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a fit() method based on the latest training loop\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_f(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1974, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling for the Training Set\n",
    "\n",
    "Training sets need to be in random order which differs for each iteration.\n",
    "\n",
    "**This does not apply to validation sets!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n, self.bs, self.shuffle = len(ds), bs, shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): \n",
    "            yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a small batch to test the sampler\n",
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without shuffling\n",
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([8, 3, 0]), tensor([6, 4, 1]), tensor([7, 2, 5]), tensor([9])]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With shuffling\n",
    "s = Sampler(small_ds, 3, True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collate() method\n",
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds, self.sampler, self.collate_fn = ds, sampler, collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler:\n",
    "            yield self.collate_fn([self.ds[i] for i in s]) # This yield, coupled with the co-routine in the sampler is an\n",
    "                                                           # example of stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[1].view(28, 28))\n",
    "yb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Since the training set is shuffled, re-running this cell will generate a different\n",
    "# output each time\n",
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[1].view(28, 28))\n",
    "yb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model and optimizer\n",
    "model, opt = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debug since this cell generates an assertion error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.2991, grad_fn=<NllLossBackward>), tensor(0.1250))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)\n",
    "#assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch's Data Loader\n",
    "\n",
    "As always, PyTorch's implementations are highly optimized and come with all the bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2221, grad_fn=<NllLossBackward>), tensor(0.9062))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can set the parameter `drop_last` to True, and it will drop the last incomplete batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model()\n",
    "fit()\n",
    "\n",
    "loss, acc = loss_f(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0885, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert acc>0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Validation loss will be printed at the end of each epoch.\n",
    "\n",
    "Also, we always call `model.train()` before training and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_f, optimizer, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        #Handle batchnorm and dropout\n",
    "        model.train()\n",
    "        #print(model.training)\n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_f(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Evaluation phase    \n",
    "        model.eval()\n",
    "        #print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc = 0., 0.\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_f(pred, yb)\n",
    "                tot_acc += accuracy(pred, yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTE` Most libraries don't have in-cuilt contingencies for different batch sizes and how they're supposed to be implemented in the fit() function above.**\n",
    "\n",
    "This will be dealt with in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function get_dls() returns dataloaders for the training and validation sets\n",
    "\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1714) tensor(0.9518)\n",
      "1 tensor(0.1522) tensor(0.9568)\n",
      "2 tensor(0.1219) tensor(0.9631)\n",
      "3 tensor(0.1047) tensor(0.9692)\n",
      "4 tensor(0.1064) tensor(0.9688)\n",
      "5 tensor(0.1176) tensor(0.9659)\n",
      "6 tensor(0.1054) tensor(0.9690)\n",
      "7 tensor(0.1118) tensor(0.9686)\n",
      "8 tensor(0.1326) tensor(0.9642)\n",
      "9 tensor(0.1282) tensor(0.9665)\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model, optimizer = get_model()\n",
    "loss, accuracy = fit(5, model, loss_f, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
