{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers - When, Where and How to Tweak Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_08 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Imagenette Data From the DataBlock NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, \n",
    "        to_float_tensor]\n",
    "bs = 128\n",
    "\n",
    "img_list = ImageList.from_files(path, tfms=tfms)\n",
    "split_data = SplitData.split_by_func(img_list, \n",
    "                                     partial(grandparent_splitter,\n",
    "                                            valid_name='val'))\n",
    "\n",
    "labels = label_by_func(split_data, parent_labeler, \n",
    "                       proc_y=CategoryProcessor())\n",
    "data = labels.to_databunch(bs, c_in=3, c_out=10, \n",
    "                           num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "nfs = [32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [partial(AvgStatsCallback, accuracy),\n",
    "             CudaCallback,\n",
    "             partial(BatchTransformXCallback, norm_imagenette)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline training with vanilla SGD\n",
    "learn, run = get_learn_run(nfs, data, lr=0.4,\n",
    "                          layer=conv_layer, cbs=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.7948843347370367, tensor(0.3808, device='cuda:0')]\n",
      "valid: [1.6955538415605096, tensor(0.4425, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTES`**\n",
    "\n",
    "- The base PyTorch optimizer in `torch.optim` is a dictionary which stores the hyper-parameters and references to the parameters of the model we want to train in parameter groups.\n",
    "\n",
    "- It contains `step` which updates our parameters with gradients and a method `zero_grad` to detach and zero the gradients of our parameters.\n",
    "\n",
    "- We will build a more flexible equivalent from scratch. Here, the step function loops over all the parameters to execute the step using stepper functions, which we will provide when initializing the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        # Generators can also be added\n",
    "        self.param_groups = list(params)\n",
    "        # Ensuring params is a list of lists\n",
    "        if not isinstance(self.param_groups[0], list):\n",
    "            self.param_groups = [self.param_groups]\n",
    "        self.hypers = [{**defaults} for p in self.param_groups]\n",
    "        self.steppers = listify(steppers)\n",
    "        \n",
    "    def grad_params(self):\n",
    "        return [(p, hyper) for pg, hyper in zip(self.param_groups, self.hypers)\n",
    "                for p in pg if p.grad is not None]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p, hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "            \n",
    "    def step(self):\n",
    "        for p, hyper in self.grad_params():\n",
    "            compose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
