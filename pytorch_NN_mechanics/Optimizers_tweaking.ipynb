{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers - When, Where and How to Tweak Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_08 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Imagenette Data From the DataBlock NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, \n",
    "        to_float_tensor]\n",
    "bs = 128\n",
    "\n",
    "img_list = ImageList.from_files(path, tfms=tfms)\n",
    "split_data = SplitData.split_by_func(img_list, \n",
    "                                     partial(grandparent_splitter,\n",
    "                                            valid_name='val'))\n",
    "\n",
    "labels = label_by_func(split_data, parent_labeler, \n",
    "                       proc_y=CategoryProcessor())\n",
    "data = labels.to_databunch(bs, c_in=3, c_out=10, \n",
    "                           num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "nfs = [32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [partial(AvgStatsCallback, accuracy),\n",
    "             CudaCallback,\n",
    "             partial(BatchTransformXCallback, norm_imagenette)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline training with vanilla SGD\n",
    "learn, run = get_learn_run(nfs, data, lr=0.4,\n",
    "                          layer=conv_layer, cbs=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.7948843347370367, tensor(0.3808, device='cuda:0')]\n",
      "valid: [1.6955538415605096, tensor(0.4425, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTES`**\n",
    "\n",
    "- The base PyTorch optimizer in `torch.optim` is a dictionary which stores the hyper-parameters and references to the parameters of the model we want to train in parameter groups.\n",
    "\n",
    "- It contains `step` which updates our parameters with gradients and a method `zero_grad` to detach and zero the gradients of our parameters.\n",
    "\n",
    "- We will build a more flexible equivalent from scratch. Here, the step function loops over all the parameters to execute the step using stepper functions, which we will provide when initializing the optimizer.\n",
    "\n",
    "- This will end up giving us parameter groups / layer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        # Could be a generator\n",
    "        self.param_groups = list(params)\n",
    "        # Ensuring params is a list of lists of parameter tensors\n",
    "        if not isinstance(self.param_groups[0], list):\n",
    "            self.param_groups = [self.param_groups]\n",
    "        # Creating a dictionary for individual param groups with\n",
    "        # their own references\n",
    "        self.hypers = [{**defaults} for p in self.param_groups]\n",
    "        self.steppers = listify(steppers)\n",
    "        \n",
    "    def grad_params(self):\n",
    "        return [(p, hyper) for pg, hyper in zip(self.param_groups, self.hypers)\n",
    "                for p in pg if p.grad is not None]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p, hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "            \n",
    "    def step(self): \n",
    "        # This step function doesn't do anything except carry out a\n",
    "        # composition on items we pass on, which in turn carry out\n",
    "        # their own operations. For e.g. one cycle annealing, \n",
    "        # discriminative LRs etc.\n",
    "        for p, hyper in self.grad_params():\n",
    "            compose(p, self.steppers, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To carry out SGD, via the stepper\n",
    "def sgd_step(param, lr, **kwargs):\n",
    "    param.data.add_(-lr, param.grad.data)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(Optimizer, steppers=[sgd_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`NOTES`**\n",
    "\n",
    "- After changing the optimizer, we will need to adjust the callbacks which used the properties from the PyTorch optimizer.\n",
    "\n",
    "- Hyper-parameters are in the list of dictionaries `opt.hypers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the Recorder, ParamScheduler and LR_Find classes\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self): self.lrs, self.losses = [], []\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if not self.in_train: \n",
    "            return\n",
    "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())\n",
    "        \n",
    "    def plot_lr(self): \n",
    "        plt.plot(self.lrs)\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.losses)\n",
    "        \n",
    "    def plot(self, skip_last=0):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        n = len(losses) - skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(self.lrs[:n], losses[:n])\n",
    "        \n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    _order = 1\n",
    "    \n",
    "    def __init__(self, pname, sched_funcs):\n",
    "        self.pname, self.sched_funcs = pname, listify(sched_funcs)\n",
    "    \n",
    "    def begin_batch(self):\n",
    "        if not self.in_train:\n",
    "            return\n",
    "        fs = self.sched_funcs\n",
    "        if len(fs)==1: \n",
    "            fs = fs*len(self.opt.param_groups)\n",
    "        pos = self.n_epochs / self.epochs\n",
    "        for f, h in zip(fs, self.opt.hypers):\n",
    "            h[self.pname] = f(pos)\n",
    "                \n",
    "\n",
    "class LR_Find(Callback):\n",
    "    _order = 1\n",
    "    def __init(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr\n",
    "        self.best_loss = 1e9\n",
    "        \n",
    "    def begin_batch(self):\n",
    "        if not self.in_train: \n",
    "            return\n",
    "        pos = self.n_iter / self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr / self.min_lr) ** pos\n",
    "        for pg in self.opt.hypers:\n",
    "            pg['lr'] = lr\n",
    "            \n",
    "    def after_step(self):\n",
    "        if self.n_iter >= self.max_iter or self.loss > self.best_loss*10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss < self.best_loss:\n",
    "            self.best_loss = self.loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
